### newsonic.py ###
# Elliott Walker #
# Last update: 4 November 2024 #
# Analysis of the snippet of sonic data #

# sample usage:
# python newsonic.py -n 8 --data="../../data/KCC_FluxData_106m_SAMPLE" --output="../../outputs/sonic_sample" --match="../../outputs/slow/ten_minutes_labeled.csv" --slow="../../outputs/slow/combined.csv"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller
import os
from pathlib import Path
from datetime import datetime
import helper_functions as hf
import multiprocessing
from mylogging import (
    Logger,
    VoidLogger
)
import warnings
warnings.filterwarnings("ignore", message=".*'DataFrame.swapaxes' is deprecated") # error generated by numpy bug

TEMPS_C = ['Ts', 'amb_tmpr'] # Original column labels containing temperatures in C (so we know which to convert to K)
IGNORE = ['CO2', 'amb_tmpr'] # Original column labels we don't care about
COLORS = [f'C{i}' for i in range(7)] # Plot color cycle
CRLATITUDE = 41.91 # Cedar Rapids latitude in degrees
WINDBOUND = 30.0 # CSAT3B sonic anemometer maximum accurate measurement of wind speed magnitude
K_WEIBULL = 1.3037 # Scaling constant for Weibull-distributed data
K_GAUSS = 1.4826 # Scaling constant for Gaussian-distributed data
THRESHOLDS = [0,25,50]

# Makes sure to properly format relative directory paths (for implicit reference of subdirectories of the working directory)
def dirpath(original):
    if '/' not in original:
        return f'./{original}'
    return original

def strip_results(results):
    ret = []
    for res in results:
        if res is not None:
            ret.append(res)
    return ret

# Loads dataframe: Handles timestamps, duplicate removal, column removal, and conversion.
def load_frame(filepath, # location of the CSV file to load
               kelvinconvert = TEMPS_C, # columns which should be converted from C -> K
               ignore = IGNORE,
               rename = {'Ux':'u','Uy':'v','Uz':'w','Ts':'T'}
               ):

    df = pd.read_csv(filepath, low_memory = False).rename(columns={'TIMESTAMP' : 'time'})

    df['time'] = pd.to_datetime(df['time'], format = 'mixed')
    df.set_index('time', inplace = True)
    df = df[~df.index.duplicated(keep = 'first')]
    df.sort_index(inplace = True)

    for col in df.columns:

        if col in ignore: # We don't care about these columns
            df.drop(columns = [col], inplace = True)
            continue
        
        df[col] = pd.to_numeric(df[col], errors = 'coerce')

        if col in kelvinconvert: # Any column listed in kelvinconvert will have its values converted from deg C to K
            df[col] += 273.15

    df.rename(columns=rename, inplace=True)

    return df

# Compute autocorrelations. Returns a dataframe of autocorrelations, timestamped by lag length.
def compute_autocorrs(df, # Dataframe to work with
                     autocols = ['u_aligned','v_aligned','w'], # Columns to compute autocorrelations for
                     maxlag = 0.5, # Work for lags from 0 up to <maxlag> * <(duration of df)>
                     logger = None
                     ):
    
    if autocols == []: # If an empty list is passed in, use all columns
        autocols = df.columns.tolist()

    kept = int(len(df)*maxlag)
    lost = len(df) - kept

    df_autocorr = pd.DataFrame(df.copy().reset_index()['time'][:kept])

    for col in autocols:

        lag_range = range(kept)

        if logger:
            logger.log(f'Autocorrelating for {col}', timestamp = True)

        Raa = []
        for lag in lag_range:
            autocorr = df[col].autocorr(lag = lag)
            Raa.append(autocorr)
        df_autocorr[f'R_{col}'] = Raa

    df_autocorr.set_index('time', inplace = True)
    df_autocorr.sort_index(inplace = True)
    starttime = df_autocorr.index[0]
    deltatime = df_autocorr.index - starttime
    df_autocorr['lag'] = deltatime.days * 24 * 3600 + deltatime.seconds + deltatime.microseconds/1e6
    df_autocorr.reset_index(drop = True)
    df_autocorr.set_index('lag', inplace = True)

    if logger:
        logger.log(f'Computed autocorrelations', timestamp = True)

    return df_autocorr

# Generate autocorrelation plots, and either save them to <saveto> or show them
def plot_autocorrs(df_autocorr,
                   title = 'Autocorrelation Plot',
                   saveto = None,
                   threshold=0.):
    
    # account for new thresholding
    
    fig, ax = plt.subplots()
    fig.suptitle(title, fontweight = 'bold')

    ax.plot(df_autocorr.index, [threshold]*len(df_autocorr), c='tab:gray', label = f'threshold = {threshold}', linewidth=1)
    if threshold != 0.:
        ax.plot(df_autocorr.index, [0.]*len(df_autocorr), c='black', linestyle='dashed', linewidth=1)

    for col in df_autocorr.columns:
        ax.plot(df_autocorr.index, df_autocorr[col], label = str(col)[2:], linewidth = 1)

    ax.set_ylim(-0.2,1.1)
    ax.set_ylabel('Autocorrelation')
    ax.set_xlabel('Lag (s)')
    ax.legend()

    fig.tight_layout(pad = 1)

    if saveto is None:
        plt.show()
    else:
        plt.savefig(saveto, bbox_inches='tight')
    plt.close()

    return

INTEGRAL_COLS = [f'u_int_t_{t}' for t in THRESHOLDS] + [f'u_int_l_{t}' for t in THRESHOLDS] + [f'w_int_t_{t}' for t in THRESHOLDS] + [f'w_int_l_{t}' for t in THRESHOLDS]
# Compute integral time and length scales
def integral_scales(df, # dataframe containing original data
                    df_autocorr, # dataframe containing the autocorrelations as computed by compute_autocorrs
                    cols = ['u_aligned','w'], # wind speed column names in <df>
                    thresholds = THRESHOLDS, # integrate up to the first time that the autocorrelation dips below each of these threshold (generating multiple results)
                    logger = None # Logger object for output
                    ):

    if 'lag' in df_autocorr.columns:
        df_autocorr.set_index('lag', inplace=True)

    dt = df_autocorr.index[1] - df_autocorr.index[0]

    result = dict()

    flag = False
    for col, name in zip(cols, ['u','w']):

        for threshold in thresholds:

            Raa = df_autocorr.reset_index()[f'R_{col}']

            mean = df[col].mean()

            cutoff_index = 0
            for i, val in enumerate(Raa):
                if val < threshold/100:
                    cutoff_index = i
                    break

            if cutoff_index == 0:
                flag = True
                if logger:
                    logger.log(f'Warning - failed to find cutoff for integration (variable {col}).')

            i_time = np.sum(Raa.loc[:cutoff_index-1]) * dt

            if i_time < 0:
                if logger:
                    logger.log(f'Warning - found negative integral time scale (variable {col})')
            i_length = abs(i_time * mean)

            result[f'{name}_int_t_{threshold}'] = i_time
            result[f'{name}_int_l_{threshold}'] = i_length

    result['autoflag'] = int(flag)

    return result

# Match computed values (bulk Ri, WSE alpha, lapse rate) from corresponding slow data
def match_results(starttime, endtime, df_match, where = ['ri', 'alpha', 'vpt_lapse_env'], how = np.median):

    dfr = df_match.reset_index()
    dfr['time'] = pd.to_datetime(dfr['time'])
    sliced = dfr[dfr['time'].between(starttime,endtime)]

    ret = dict()

    for w in where:
        ret[w] = how(sliced[w])

    return ret

# Determine mean wind direction (direction of vector average wind)
def mean_direction(df):

    ux = df['u']
    uy = df['v']

    uxavg = np.mean(ux)
    uyavg = np.mean(uy)
    dir_to_align = np.arctan2(uyavg, uxavg)

    return dir_to_align

# Geometrically align the Ux and Uy components of wind such that Ux is oriented in the direction of the mean wind and Uy is in the crosswind direction
def align_to_direction(df, dir_to_align):

    ux = df['u']
    uy = df['v']

    ux_aligned = ux * np.cos(dir_to_align) + uy * np.sin(dir_to_align)
    uy_aligned = - ux * np.sin(dir_to_align) + uy * np.cos(dir_to_align)

    dfc = df.copy()
    dfc['u_aligned'] = ux_aligned
    dfc['v_aligned'] = uy_aligned

    return dfc

# Plot wind speed data
def plot_data(df,
              title = 'Wind Plot',
              saveto = None,
              cols = ['u','v','w'],
              df_slow = None):
    
    # need to update and use
    # once unaligned
    # once aligned
    # maybe also plot temperatures? could also get overlaid temperatures from slow?? separate function???
    
    fig, ax = plt.subplots()
    fig.suptitle(title, fontweight = 'bold')

    starttime = df.index[0]
    deltatime = df.index - starttime
    deltaseconds = hf.seconds(deltatime)

    if df_slow is not None:
        slowtime = df_slow.index - starttime
        slowseconds = hf.seconds(slowtime)

    for col, color in zip(cols, COLORS):
        if col not in df.columns:
            continue
        ax.plot(deltaseconds, df[col], label = str(col), linewidth = 1, c = color)
        if (df_slow is not None) and col in df_slow.columns:
            ax.scatter(slowseconds, df_slow[col], s = 20, c = color, edgecolors='black', zorder=10) # overlay slow data
    
    ax.set_ylabel('Wind speed (m/s)')
    ax.set_xlabel(f'Seconds since {starttime}')
    ax.legend()

    fig.tight_layout(pad = 1)

    if saveto is None:
        plt.show()
    else:
        plt.savefig(saveto, bbox_inches='tight')
    plt.close()

    return

def missing_keys(dictionary, keys):
    # Returns True if any key in `keys` is not a key in `dictionary`, and False otherwise (if keys is a subset of the dict's keys)
    for key in keys:
        if key not in dictionary.keys():
            return True
    return False

def missing_cols(dataframe, cols):
    # like missing_keys but for column labels in a dataframe
    if dataframe is None:
        return True
    for col in cols:
        if col not in dataframe.columns:
            return True
    return False

DF_FLUX_COLS = ["u'","v'","w'","T'"] # ["vpT","u'","v'","w'","T'","vpT'"]
#DERIVED_COLS = ['u_raw_mean','v_raw_mean','u_mean','v_mean','w_mean','T_mean','vpT_mean','u_rms','v_rms','w_rms',"T_rms","vpT_rms","u'w'_mean","v'w'_mean","T'w'_mean","vpT'w'_mean","u_star","L","Ri_flux","dudz","TKE"]
DERIVED_COLS = ['u_raw_mean','v_raw_mean','u_mean','v_mean','w_mean','T_mean','u_rms','v_rms','w_rms',"T_rms","u'w'_mean","v'w'_mean","T'w'_mean","u_star","L","Ri_flux","dudz","TKE"]
def compute_fluxes(df):

    dfc = df.copy()

    # compute sonic virtual potential temperature
    # dfc['vpT'] = dfc.apply(lambda row: hf.alternate_virtual_potential_temperature(row['H2O']+, row['amb_press'], row['T']), axis = 1)

    mean_u_raw = dfc['u'].mean() # compute means on unaligned wind data too
    mean_v_raw = dfc['v'].mean()

    mean_u = dfc['u_aligned'].mean() # compute means on aligned wind data
    mean_v = dfc['v_aligned'].mean() # the aligned data will be used for the rest of this
    mean_w = dfc['w'].mean()
    mean_T = dfc['T'].mean()
    #mean_vpT = dfc['vpT'].mean()

    flux_u = dfc['u_aligned'] - mean_u
    flux_v = dfc['v_aligned'] - mean_v
    flux_w = dfc['w'] - mean_w
    flux_T = dfc['T'] - mean_T
    #flux_vpT = dfc['vpT'] - mean_vpT

    dfc["u'"] = flux_u
    dfc["v'"] = flux_v
    dfc["w'"] = flux_w
    dfc["T'"] = flux_T
    #dfc["vpT'"] = flux_vpT

    uw = flux_u * flux_w
    vw = flux_v * flux_w
    Tw = flux_T * flux_w
    #vpTw = flux_vpT * flux_w

    uu = flux_u * flux_u
    vv = flux_v * flux_v
    ww = flux_w * flux_w

    uw_mean = uw.mean()
    vw_mean = vw.mean()
    Tw_mean = Tw.mean()
    #vpTw_mean = vpTw.mean()
    
    uu_mean = uu.mean()
    vv_mean = vv.mean()
    ww_mean = ww.mean()
    u_rms = np.sqrt(uu_mean)
    v_rms = np.sqrt(vv_mean)
    w_rms = np.sqrt(ww_mean)
    T_rms = np.sqrt((flux_T * flux_T).mean())
    #vpT_rms = np.sqrt((flux_vpT * flux_vpT).mean())

    u_star = (uw_mean**2 + vw_mean**2)**(1/4)
    obukhov_length = hf.obukhov_length(u_star, mean_T, Tw_mean) #hf.obukhov_length(u_star, mean_vpT, vpTw_mean)
    flux_ri, vertical_wind_gradient = hf.flux_richardson(uw_mean, mean_T, Tw_mean, u_star, report_gradient=True) # hf.flux_richardson(uw_mean, mean_vpT, vpTw_mean, u_star, report_gradient=True)

    tke = (uu_mean + vv_mean + ww_mean)/2

    derived = {
        "u_raw_mean" : mean_u_raw,
        "v_raw_mean" : mean_v_raw,
        "u_mean" : mean_u,
        "v_mean" : mean_v,
        "w_mean" : mean_w,
        "T_mean" : mean_T,
        #"vpT_mean" : mean_vpT,
        "u_rms" : u_rms,
        "v_rms" : v_rms,
        "w_rms" : w_rms,
        "T_rms" : T_rms,
        #"vpT_rms" : vpT_rms,
        "u'w'_mean" : uw_mean,
        "v'w'_mean" : vw_mean,
        "T'w'_mean" : Tw_mean,
        #"vpT'w'_mean" : vpTw_mean,
        "u_star" : u_star,
        "L" : obukhov_length,
        "Ri_flux" : flux_ri,
        "dudz" : vertical_wind_gradient,
        "TKE" : tke
    }

    return dfc, derived

SPOLETO_COLS = ['instation','itc_dev','sflag']
def covariance(df, cols = ['u_aligned', 'w']): # compute covariance
    sumxz = np.sum(df[cols[0]]*df[cols[1]])
    sumx = np.sum(df[cols[0]])
    sumz = np.sum(df[cols[1]])
    result = (sumxz - (sumx * sumz)/len(df))/(len(df) - 1)
    return result

def covar_instationarity(df, subintervals = 6): # compute relative instationarity according to Foken & Wichura (1996)
    # covariance of Ux and Uz (u and w) winds computed as average of that among subintervals
    covs = []
    subdfs = np.array_split(df, subintervals)
    for subdf in subdfs:
        covs.append(covariance(subdf))
    meancov = np.mean(covs)
    # same covariance computed across full interval
    fullcov = covariance(df)
    # compute instationarity as relative difference between the two
    instationarity = np.abs((meancov-fullcov)/fullcov)
    return instationarity

def compute_itc_deviation(df, z, ustar, L, lat, cols = ['u_aligned','w']): # compute integral turbulence characteristic deviation based on Tables 6 and 7 of Mauder & Foken (2011)
    f = hf.coriolis(lat)
    zoverL = z/L
    if -0.2 < zoverL < 0.4:
        model_u = 0.44*np.log(f/ustar) + 6.3
        model_w = 0.21*np.log(f/ustar) + 3.1
    else:
        model_u = 2.7*np.abs(zoverL)**(1/8)
        model_w = 2.0*np.abs(zoverL)**(1/8)
    measured_u = np.std(df[cols[0]])/ustar
    measured_w = np.std(df[cols[1]])/ustar
    dev_u = np.abs((model_u-measured_u)/model_u)
    dev_w = np.abs((model_w-measured_w)/model_w)
    return max(dev_u, dev_w)

def adf_test(df, which = ['u', 'v', 'w']):
    fails = 0
    critical_fails = 2 if len(which) > 1 else 1
    for col in which:
        dftest = adfuller(df[col], autolag = "AIC")
        statistic = dftest[0]
        pval = dftest[1]
        critical = dftest[4]['5%']
        if statistic > critical or pval > 0.05:
            fails += 1
        if fails >= critical_fails:
            return 2
    if fails == 0:
        return 0
    return 1

def spoleto(instation, itcdev): # return Spoleto agreement flag (0, 1, or 2) based on Table 16 of Mauder & Foken (2011)
    keychar = max(instation, itcdev)
    if keychar < 0.3:
        return 0 # high quality data
    elif keychar < 1.:
        return 1 # moderate quality data
    else:
        return 2 # low quality data

def spoleto_check(df, height, latitude, ustar, obukhov):
    covar_instation = covar_instationarity(df)
    itc_deviation = compute_itc_deviation(df, z=height, lat=latitude, ustar=ustar, L=obukhov)
    spoleto_flag = spoleto(covar_instation, itc_deviation)
    ret = {
        'instation' : covar_instation,
        'itc_dev' : itc_deviation,
        'sflag' : spoleto_flag
    }
    return ret

def bounds_qc(df, bound = WINDBOUND):
    
    removals = 0

    outliers = np.sqrt(df['u']**2 + df['v']**2 + df['w']**2) > bound
    removals += df[outliers].shape[0]
    df = df[~outliers]

    return df, removals

def mad_qc(df, cols = ['u', 'v', 'w'], n = 5, K = K_WEIBULL):

    removals = 0

    for col in cols:
        med = np.median(df[col])
        MAD = np.median(np.abs(df[col] - med))
        threshold = n * K * MAD
        outliers = np.abs(df[col] - med) > threshold
        removals += df[outliers].shape[0]
        df = df[~outliers]

    return df, removals

def hold_qc(df, cols = ['u', 'v', 'w'], m = 3):

    def constant_list(mylist):
        for i in range(len(mylist)):
            if mylist[i] != mylist[i-1]: return False
        return True

    dfc = df.reset_index()
    
    bad_indices = set()

    for col in cols:
        series = df[col]
        lastM = ["X" for _ in range(m)]
        for i, val in enumerate(series):
            lastM = lastM[1:] + [val]
            if constant_list(lastM):
                bad_indices = bad_indices.union({i-j for j in range(m)})

    df = dfc.drop(labels = bad_indices, axis = 0).set_index('time')

    return df, len(bad_indices)

def nan_qc(df, cols = ['u', 'v', 'w']):

    initial = len(df)

    df = df.dropna(axis = 0, how = 'any', subset = cols)

    removals = initial - len(df)

    return df, removals

def quality_control(df, logger):

    removals = 0

    df, bound_removals = bounds_qc(df)
    if bound_removals > 0:
        removals += bound_removals
        logger.log(f'Measurement bounds check resulted in {bound_removals} removals')

    df, mad_removals = mad_qc(df)
    if mad_removals > 0:
        removals += mad_removals
        logger.log(f'MAD spike detection resulted in {mad_removals} removals')

    df, hold_removals = hold_qc(df)
    if hold_removals > 0:
        removals += hold_removals
        logger.log(f'Hold detection resulted in {hold_removals} removals')

    df, nan_removals = nan_qc(df)
    if nan_removals > 0:
        removals += nan_removals
        logger.log(f'NaN removals resulted in {nan_removals} removals')

    logger.log(f'Completed preliminary QC with {removals} total removals ({(removals/len(df)):.4f}%)')

    return df, removals

def _analyze_file(arguments):
    try:
        filename, args, identifier = arguments

        logger = args['logger'].sublogger()

        path = os.path.abspath(os.path.join(parent,filename))
        if not(os.path.isfile(path) and filename[-4:] == '.csv'):
            logger.log(f'Fatal error: improper format for {path}')
            return
        logger.log(f'Loading {path} (id {identifier})', timestamp = True)
        df = load_frame(path)
        #df['time'] = pd.to_datetime(df['time'])
        #df.set_index('time', inplace=True)

        result = dict()

        name = filename[:-4]
        result['filename'] = name

        plotdir = os.path.join(savedir, 'plots/')
        progdir = os.path.join(savedir, 'progress/')
        progfile = os.path.join(progdir, f'computed_{filename}')
        acdir = os.path.join(savedir, 'autocorrs/')
        acfile = os.path.join(acdir, f'autocorrs_{filename}')
        sumfile = os.path.join(savedir, 'summary.csv')

        if (not args['restart']) and Path(progfile).is_file():
            # if progfile (df with progress made) exists, read and use it
            df_progress = pd.read_csv(progfile)
        else:
            df_progress = None

        if (not args['restart']) and Path(sumfile).is_file():
            # if sumfile (summary file) exists, use part of it
            try:
                df_summary = pd.read_csv(sumfile).set_index('filename')
                candidate = df_summary.loc[[name]].reset_index()
                using = True
                logger.log(f'Using some cached summary file results for {name}')
            except:
                using = False
                logger.log(f'Failed to load cached summary file {sumfile}')
            if using:
                result = candidate.to_dict(orient='records')[0]

        starttime = df.index[0]
        endtime = df.index[-1]
        logger.log(f'Time interval: {starttime} to {endtime}')
        if missing_keys(result, ['start','end']):
            result['start'] = starttime
            result['end'] = endtime

        # initial qc
        if missing_keys(result, ['removed']):
            df, removals = quality_control(df, logger = logger)
            result['removed'] = removals

        df_slow = args['slow'].copy()
        df_slow = df_slow[df_slow['time'].between(starttime, endtime)]
        df_slow.set_index('time', inplace = True)
        df_slow[['u', 'v']] = df_slow.apply(lambda row: hf.wind_components(row['ws_106m'], row['wd_106m'], invert=True), axis=1, result_type='expand') # convert to east and north components
        df_slow.rename(columns = {'wd_106m':'direction'})
        df_slow.drop(columns = ['ws_106m','wd_106m'], inplace=True)
        logger.log('Matched corresponding slow data at 106m')

        df['direction'] = df.apply(lambda row: hf.polar_wind(row['u'],row['v'])[1], axis = 1)
        dir_to_align = mean_direction(df) # determine direction of mean wind
        result['mean_dir'] = np.rad2deg(dir_to_align)
        df = align_to_direction(df, dir_to_align) # align data to direction of mean wind
        logger.log('Aligned data: Ux oriented in direction of mean wind')
        result['delta_dir'] = np.rad2deg(dir_to_align - mean_direction(df_slow))
        df_slow = align_to_direction(df_slow, dir_to_align) # also align the slow data to the same direction, if it exists
        logger.log('Aligned slow data to match orientation of sonic data')

        if missing_keys(result, ['Ri_bulk','alpha','vpt_lapse_env']):
            matched = match_results(starttime, endtime, args['matched'])
            result['Ri_bulk'] = matched['ri']
            result['alpha'] = matched['alpha']
            result['vpt_lapse_env'] = matched['vpt_lapse_env']

        # compute fluxes
        if missing_keys(result, DERIVED_COLS) or missing_cols(df_progress, DF_FLUX_COLS):
            logger.log('Conducting flux computations')
            df, derived = compute_fluxes(df)
            result = result | derived
        else:
            logger.log('Flux data already computed')
            for col in DF_FLUX_COLS:
                df[col] = df_progress[col]

        # compute autocorrelations
        # look for preexisting???
        using_ac = False
        if (not args['restart']) and Path(acfile).is_file():
            try:
                candidate = pd.read_csv(acfile).set_index('lag')
                df_autocorr = candidate
                using_ac = True
                logger.log(f'Using cached autocorrelations for {name}')
            except:
                logger.log(f'Failed to load cached autocorrelations {acfile}')                
        if not using_ac:
            df_autocorr = compute_autocorrs(df, maxlag=args['maxlag'], logger=logger)
            df_autocorr.to_csv(acfile)

        # compute integral time and length scales
        if (not args['restart']) and missing_keys(result, INTEGRAL_COLS):
            logger.log('Computing integral time and length scales')
            integral = integral_scales(df, df_autocorr, logger = logger)
            result = result | integral
        else:
            logger.log(f'Using cached integral scales for {name}')

        # save wind speed plots, both unaligned and aligned

        # save wind direction plots

        # save flux plots

        # save autocorrelation plots

        # spoleto agreement flag
        if (not args['restart']) and missing_keys(result, SPOLETO_COLS):
            spoleto = spoleto_check(df, args['height'], args['latitude'], result['u_star'], result['L'])
            result = result | spoleto

        # adf test flag
        if args['adf']:
            logger.log('Conducting ADF test.')
            adf = adf_test(df)
            logger.log(f'Resulting ADF flag: {adf}')
            result['adfflag'] = adf

        df.to_csv(progfile)

        return result
    
    except Exception as e:
        if logger is not None and type(logger) is Logger:
            logger.log(f'Exception encountered: {e}')
        else:
            print(f'Exception encountered: {e}')

        return
    
def analyze_directory(parent,
                      maxlag,
                      nproc,
                      logger,
                      height,
                      latitude,
                      savedir,
                      matchfile,
                      slowfile,
                      restart,
                      adf
                      ):

    summaryfile = os.path.join(savedir, 'summary.csv')

    if type(nproc) is int and nproc > 1 and not adf:
        logger.log(f'MULTIPROCESSING ENABLED: {nproc=}')
    else:
        logger.log('Multiprocessing DISABLED.')
        nproc = 1
    
    df_match = pd.read_csv(matchfile)
    df_match.set_index('time', inplace = True)
    df_slow = pd.read_csv(slowfile)
    df_slow = df_slow[['time','ws_106m','wd_106m']] # select only the 106m data from the slow file. this does for now require the specific formatting and data height for the slow data.
    df_slow['time'] = pd.to_datetime(df_slow['time'])

    args = {'parent' : parent,
            'maxlag' : maxlag,
            'logger' : logger,
            'height' : height,
            'latitude' : latitude,
            'savedir' : savedir,
            'matched' : df_match,
            'slow' : df_slow,
            'restart' : restart,
            'adf' : adf
            }
    directory = [(filename, args, i) for i, filename in enumerate(os.listdir(parent))]
    
    plotdir = os.path.join(savedir, 'plots/')
    progdir = os.path.join(savedir, 'progress/')
    acdir = os.path.join(savedir, 'autocorrs/')
    for intermediate in (plotdir, progdir, acdir):
        os.makedirs(intermediate, exist_ok = True)

    pool = multiprocessing.Pool(processes = nproc)
    
    # Use pool.map to distribute the work
    results = pool.map(_analyze_file, directory) # this will be a list of dicts
    pool.close()
    pool.join()

    summarypath = os.path.abspath(os.path.join(savedir, 'summary.csv'))
    logger.log(f'Finished analysis. Now saving data to {summarypath}')
    summary = pd.DataFrame(strip_results(results)).set_index('filename') # convert the resulting list of dicts to a dataframe for easy CSV saving
    summary.to_csv(summarypath, float_format='%.6f') # save summary of results as CSV

    logger.log(f'COMPLETED!', timestamp = True)

    return

class SonicResults:
    def __init__(self, savedir):
        self.savedir = os.path.abspath(savedir)
    
    def get_summary(self):
        df = pd.read_csv(os.path.join(savedir, 'summary.csv'))
        df['start'] = pd.to_datetime(df['start'])
        df['end'] = pd.to_datetime(df['end'])
        df.set_index('start', inplace = True)
        df.sort_index(inplace = True)
        return df

    def list_files(self):
        return
    
    def view_plots(self, type='raw'):
        # type may be 'raw', 'aligned', 'fluxes', 'autocorrelations'
        return

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        prog = 'sonic.py',
        description = 'Analyzes chunks of sonic data',
    )

    parser.add_argument('-d', '--data', default = '../../data/KCC_FluxData_106m_SAMPLE', help = 'input data directory')
    parser.add_argument('-o', '--output', default = '../../outputs/sonic_sample',  help = 'output target directory')
    parser.add_argument('-m', '--match', default = '../../outputs/slow/ten_minutes_labeled.csv', help = 'file containing bulk Ri to match')
    parser.add_argument('-s', '--slow', default = '../../outputs/slow/combined.csv', help = 'file containing slow data to match')
    parser.add_argument('-n', '--nproc', default = '1', help = 'number of CPUs to use')
    parser.add_argument('-r', '--restart', action = 'store_true', help = 'restart all computations, deleting any existing intermediate computations')
    parser.add_argument('--latitude', default=str(CRLATITUDE), help = 'site latitude, in degrees')
    parser.add_argument('--height', default='106', help = 'height (z) of data collection, in meters')
    parser.add_argument('--nolog', action = 'store_true', help = 'do not output a log file?')
    parser.add_argument('--adf', action = 'store_true', help='conduct ADF test? (Incompatible with multiprocessing)')

    args = parser.parse_args()

    parent = dirpath(args.data)
    savedir = dirpath(args.output)
    matchfile = dirpath(args.match)
    slowfile = dirpath(args.slow)

    os.makedirs(os.path.join(savedir, 'logs'), exist_ok = True)

    if args.nolog:
        logger = VoidLogger()
    else:  
        timeid = datetime.now().strftime("%Y%m%dT%H%M%S") # ISO 8601 date and time format
        logfile = os.path.join(savedir, 'logs', f'sonic_analysis_{timeid}.log')
        logger = Logger(logfile = logfile)

    # Conduct analysis with options set
    analyze_directory(parent = parent,
                      maxlag = 0.5,
                      nproc = int(args.nproc),
                      logger = logger,
                      height = float(args.height),
                      latitude = float(args.latitude),
                      savedir = savedir,
                      matchfile = matchfile,
                      slowfile = slowfile,
                      restart = args.restart,
                      adf = args.adf
                      )
